{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinic_directory = 'C:/Users/Dell/.cache/kagglehub/datasets/mengcius/cinic10/versions/1'\n",
    "cinic_mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "cinic_std = [0.24205776, 0.23828046, 0.25874835]\n",
    "cinic_train = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(cinic_directory + '/train',\n",
    "    \ttransform=transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cinic_mean,std=cinic_std)])),\n",
    "    batch_size=128, shuffle=True)\n",
    "cinic_valid = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(cinic_directory + '/valid',\n",
    "    \ttransform=transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cinic_mean,std=cinic_std)])),\n",
    "    batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 50x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etykieta: 0\n"
     ]
    }
   ],
   "source": [
    "cinic_mean_tensor = torch.tensor(cinic_mean).view(3, 1, 1)\n",
    "cinic_std_tensor = torch.tensor(cinic_std).view(3, 1, 1)\n",
    "\n",
    "def imshow(img):\n",
    "    print(img.size())\n",
    "    img = img * cinic_std_tensor + cinic_mean_tensor  # Odwracamy normalizacjÄ™\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(0.5,0.5))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Pobranie batcha obrazÃ³w\n",
    "data_iter = iter(cinic_train)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# WyÅ›wietlenie pierwszego obrazu\n",
    "imshow(images[0])\n",
    "print(\"Etykieta:\", labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Redukcja rozmiaru 32x32 -> 16x16\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Redukcja rozmiaru 16x16 -> 8x8\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)  # CINIC-10 ma 10 klas\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGStyleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGStyleCNN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Redukcja rozmiaru 32x32 -> 16x16\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Redukcja rozmiaru 16x16 -> 8x8\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Redukcja rozmiaru 8x8 -> 4x4\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Redukcja rozmiaru 4x4 -> 2x2\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 2 * 2, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 10)  # CINIC-10 ma 10 klas\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoka 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [05:52<00:00,  1.99it/s, loss=0.472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoka [1/2], Strata: 0.4632, DokÅ‚adnoÅ›Ä‡: 83.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoka 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [07:30<00:00,  1.56it/s, loss=0.318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoka [2/2], Strata: 0.4222, DokÅ‚adnoÅ›Ä‡: 84.86%\n",
      "âœ… Trenowanie zakoÅ„czone!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGGStyleCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 7\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(cinic_train, desc=f\"Epoka {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Obliczanie dokÅ‚adnoÅ›ci na zbiorze treningowym\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = running_loss / len(cinic_train)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"ðŸ“Š Epoka [{epoch+1}/{num_epochs}], Strata: {avg_loss:.4f}, DokÅ‚adnoÅ›Ä‡: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"âœ… Trenowanie zakoÅ„czone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DokÅ‚adnoÅ›Ä‡ na zbiorze walidacyjnym: 73.87%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in cinic_valid:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"DokÅ‚adnoÅ›Ä‡ na zbiorze walidacyjnym: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
